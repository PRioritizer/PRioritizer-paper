\section{Study Design}
\label{sec:design}

To anwser our research questions we used different research techniques.
We used quantitative data sources, proved by design and performed a small qualitative study.
The sections describes the methods in detail.

\subsection{Data}
All the data used for prioritization originates from GitHub.
However, it is not trivial to extract large amounts of data from GitHub.
Although they offer an \api\footnote{https://developer.github.com/v3} to fetch various kinds of data, they limit the use at a rate of 5,000 \api calls per hour for a single user.

To overcome this limitation and enable mining large portions of GitHub's historical data, the \ghtorrent project \cite{Gousi13} was initiated.
It provides a scalable, queryable, offline mirror of the data offered through the GitHub \api.
By monitoring GitHub's public event stream and storing those events, \ghtorrent harvests the data.
Since the beginning of the project in 2012, \ghtorrent already collected more than 5 terabytes of data.
We use \ghtorrent for getting large amounts of data for the prioritization process.

\subsection{Implementation}
We wanted to prove by design whether it is possible to automatically prioritize PRs.
Therefore we implemented a preliminary service, the \prioritizer service.
It uses the \ghtorrent project extensively to extract data and machine learning to prioritize PRs based on a project's historical data.
The service was developed in the Scala programming language and uses R for the machine learning part.
As the service requires that the repositories it prioritizes are cloned on the server, it requires a lot of diskspace.
This is why the service is deployed and being tested on the \ghtorrent servers.
The prioritized repositories are available via the \ghtorrent project website \footnote{http://ghtorrent.org/prioritizer/\#/display/[owner]/[repo]/[secret]}.
By using the \ghtorrent's pull request event queue, the prioritization of each repository can be kept up-to-date.

\subsection{Participants}
To evaluate the preliminary service we build, we tried to perform a qualitative study to the service's performance.
We invited individual integrators to test the service on their own repositories and to fill in a short evaluation form.
The selection of the participants was divided into two groups.

The first group of integrators was obtained by recycling a list of people used by a previous questionnaire \cite{GZSD15}.
Everyone who previously had indicated that they were available for a follow-up, were included in the first group.
This gave us a group of 265 integrators.
The repositories (244) they manage were also known from the questionnaire and were added to the prioritization service.

The participants received an email containing a direct link to their up-to-date prioritized list of open PRs and a link to the evaluation form.
Unfortunately, the response rate was not high.
We received only 9 (3\%) forms that were filled in.
Because of the low response rate we decided to contact a another group of people.

The second group of people consisted of integrators of repositories that have a large amount of open PRs or a high average new PRs per day.
This list of repositories was obtained by querying \ghtorrent.
After we manually removed some repositories that seemed bogus or were used for test purposes, 148 repositories remained.
Again, we cloned each of these repositories so that the prioritization service built an initial ranking.
For each repository we queried \ghtorrent to get the top 3 integrators.
Without duplicates from the first round, it left us with a group of 304 integrators.
Because these repositories have a large amount of PRs, these integrators would feel the pain of prioritizing PRs manually the most.

To encourage people to respond more, we included in our email a performance report \footnote{http://ghtorrent.org/pullreq-perf/} of their repository and an opportunity to get a \$50 Amazon voucher.
This time we received 12 answers (4\%), again a low response rate.
We also received 10 replies by email, 4 of those contained actual additional feedback.
Which brings the total number of responses to 25.
