\section{Implementation}
\label{sec:implementation}

To test whether it is possible to prioritize PRs we have designed a service that automatically tries to do just that.

\subsection{Features}
\label{sec:features}

For creating system that automatically creates an ordered list of PRs, we tried to use a machine algorithm that ranks the incoming PRs according a set of features.
Most of the features we use are based on features that are used by many integrators to manually sort their PRs \cite{GZSD15}.

\begin{description}
\item[Age]
The number of minutes between the moment the PR was opened to the current time.
There is a large group of integrators that take a PR's age into account when manually prioritizing their PRs.

\item[Target Branch]
The name of the branch the PR is targeting in the base repository.
It makes sense to use the target branch not as string, but as a categorical variable.
Unfortunately the ML algorithm chokes if there is a new branch value that was not part of the training data.
Therefore this feature is not included in the training data at all, but it is still available for manual sorting (see visualizer in section~\ref{sec:architecture}).

\item[Core Member]
A boolean value indicating whether the author of the pull request is a core member of the project.
Some integrators value PRs of members more than those of non-members.

\item[Intra-Branch]
A boolean value indicating whether the pull request is coming from another branch of the same base project.
Creating these kind of PRs requires write access to the repository, so they can only be created by core members.

\item[Contains Fix]
A boolean value indicating whether the pull request contains some sort of bug fix rather than e.g. a feature.
This is really just an indication as the value is only true if the PR contains the string ``fix'' in its title.
In general PRs with a fix are prioritized over other PRs.

\item[Contribution Rate]
The number of commits that is already included in the project \emph{and} authored by the pull requester divided by the total number of commits in the project.
I.e. the contribution rate of the PR author.
There are integrators that look at PRs of known contributers first, their code is of known quality and often faster to review.
On the other hand there are also integrators that value PRs of new contributors more than those of known contributers.
The rationale behind this is that new contributers are feeling more valued when they receive a fast response on their PR.
Possibly will that feeling encourage them to continue to contribute the to project.

\item[Accept rate]
The number of accepted (i.e. merged) pull requests authored by the current pull requester divided by the total number of PRs he/she authored.
I.e. the accept rate of the PR author.
A high accept rate can indicate that the author already gained some trust by the integrator, which can result in a higher prioritity.
This feature suffers possibly from accuracy problems, as it is difficult to determine whether a PR is actually merged or not.
A pull request can be merged in different ways \cite{GPD14}.
A merge performed via the GitHub interface is easy to detect, but a PR which is squashed to one commit before the merge is difficult to detect.

\item[Comments]
There are different types of comments that can be made regarding a pull request.
This feature counts the number of comments on the issue that is automatically created along the pull request.
Usually a general dicussion about the PR takes place within the issue.
The number of comments might indicate whether a PR needs more/less attention.

\item[Review Comments]
Review comments are comments on a portion of the unified diff of a pull request.
These are separate from issue comments, which do not reference a portion of the unified diff.
This feature counts the number of review comments on a PR.

\item[Last Comment Mention]
A boolean value indicating whether the last comment on a request (regardless of the comment type) contains a mention to a GitHub user.
A comment with a mention contains ``@username'' anywhere in the body of the comment.
The mention suggests that the user who is mentioned needs to take action.

\item[Additions]
The number of lines added by the pull request.
Together with the following three features it captures the size of a PR.
A very large group uses the size of the change to manually prioritize their PRs.

\item[Deletions]
The number of lines deleted by the pull request.

\item[Commits]
The number of commits contained by the pull request.

\item[Files]
The number of files changed by the pull request.

\item[Has Test Code]
A boolean value indicating whether the pull request contains changes to test files.
Some projects have a extensive test suite.
In these projects it is often expected that new PRs contain tests for the code they add.
The heuristic used for the test code detection is simple, the value of the feature is true if the PR changes at least one file with ``test'' or ``spec'' in its file name.

\item[Pairwise conflicts]
When the conflicts among other pending PRs are known, it can be used to determine a merge order in which the number of conflicts is reduced.
The actual number of conflicts is not reduced, but they can be concentrated in a particular merge action.
Figure~\ref{fig:conflicts-1} and \ref{fig:conflicts-3} show an example of the same set of three PRs but with different merge orders which result in a different amount of merge conflicts.
Pairwise conflicts are only checked among PRs which target the same branch, but nevertheless the amount of pairs can be very high.
In the worst case, i.e. when all PRs target the same branch, the number of pairs to be checked is $O(n^2)$.

A possible value of this feature could be the number of conflicts between other pending PRs.
However, since checking the pairwise conflicts takes quadratic time it would take a large amount of time to create the training data which requires multiple time windows for each pull request.
It boils down to $O(n^2 \cdot m)$ where $m$ is the average number of time windows per PR.
Because of this time complexity, we decided to leave this feature out of the ML algorithm and only make it available for manual sorting.

\begin{figure}
  \centering
  % Trim option's parameter order: left bottom right top
  \includegraphics[height=25mm, clip ,trim = 0mm 7mm 0mm 7mm]{../figs/conflicts-1.pdf}
  \caption[Merge diagram with one conflict]
   {Merge diagram with one merge conflict.
   Consider the following conflicted commit pairs: $(2,7)$, $(3,8)$, $(5,9)$.
   The conflicts occur only in merge commit $10$.}
  \label{fig:conflicts-1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[height=25mm, clip ,trim = 0mm 7mm 0mm 7mm]{../figs/conflicts-3.pdf}
  \caption[Merge diagram with three merge conflicts]
   {Merge diagram with three merge conflicts.
   Consider the following conflicted commit pairs: $(2,7)$, $(3,8)$, $(5,9)$.
   The conflicts occur in each of the three merge commits $10,4,6$.}
  \label{fig:conflicts-3}
\end{figure}

\end{description}

\subsection{Training data}
\label{sec:training}

With the features described in the previous section we construct the training data.
For reasons discussed earlier we do not include the \emph{target branch} and \emph{pairwise conflicts} features.
There are different approaches of applying the pull-based development model \cite{GPD14}.
Because of this we create a different training set for every different GitHub project.

To train a prediction model so that it recognizes PRs that need attention, we have to provide it with actual examples of pull requests that we consider important.
We define important pull requests as follows: the current state of a pull request is important when an action follows on that PR.
An action can either be a merge, close, comment or review comment action.
To find out what the important states were, we have to dig into the past of previously closed PRs.
All the available information about the closed pull requests is extracted from \ghtorrent.
To determine the state of PRs in the past we take a systematic approach by dividing the lifetime of each PR into time windows.
The windows, with an interval of a day, contain a snapshot of the PR at the beginning of the window.
A PR's snapshot consists of its commits, comments and author info at the time.
Now that every PR is broken down into a list of consectutive snapshots, we can add information about the importance.
When an action is carried out within a certain time window, the snapshot contained in that window is marked as \emph{important}.
Snapshots of time windows that don't have any actions are not marked as important.
The final training data consists of these snapshots, where each snapshot is a row in the training data.

\subsection{Modeling}
\label{sec:modeling}

We used the training data of 20 different projects on three commonly used machine learning algorithms.
The projects varied in size and the algorithms we tested are: Logistic Regression, Naive Bayes and Random Forest.
We ran the algorithm against each of the 20 projects with a 10-fold cross-validation.
The models were trained with 90\% of the training data, the remaining 10\% was used as test data.

In table~\ref{tab:alg-compare} it can be seen how the three algorithms performed on average.
Both LR and NB perform well on recall, but have a very low precision which makes it unusable for our application.
The RF algorithm performs less on recall, but achieves a higher precision, which is more important in our case.
High precision means that the algorithm returned substantially more relevant results than irrelevant.
It seems that RF is the best at identifying the important pull requests.

\begin{table}
  \begin{tabular}{ l | c | c | c | c | c | c }
    Alg. & $\overline{p}$ & $\sigma(p)$ & $\overline{r}$ & $\sigma(r)$ & $\overline{F1}$ & $\sigma(F1)$ \\ \hline
    \hline
    LR & .2423 & .1721 & .8377 & .0581 & .3455 & .1925 \\ \hline
    NB & .2219 & .1527 & .8048 & .0704 & .3198 & .1718 \\ \hline
    RF & .6885 & .1120 & .4979 & .1270 & .5715 & .1157 \\
  \end{tabular}
  \caption[Comparision of algorithms]{Comparision of algorithms. The average precision, recall and F1-score for each algorithm. }
  \label{tab:alg-compare}
\end{table}

The results in table~\ref{tab:alg-compare} led us believe that there was room for improvement.
Since the important snapshot data vs. unimportant data is very imbalanced, we decided to tune the balance.
By restricting the number of unimportant snapshots (F) to a multiple of the important part (T) we obtained the averaged results in table~\ref{tab:balance}.
The four different balanced ratios were applied in 10-fold for each project.

\begin{table}
  \begin{tabular}{ l | c | c | c | c | c | c }
    F:T & $\overline{p}$ & $\sigma(p)$ & $\overline{r}$ & $\sigma(r)$ & $\overline{F1}$ & $\sigma(F1)$ \\ \hline
    \hline
    4F:1T & .6001 & .1676 & .5769 & .1047 & .5721 & .1141 \\ \hline
    3F:1T & .5674 & .1787 & .6015 & .0939 & .5658 & .1206 \\ \hline
    2F:1T & .5235 & .1965 & .6549 & .0945 & .5564 & .1475 \\ \hline
    1F:1T & .4586 & .2450 & .7630 & .0600 & .5376 & .2030 \\
  \end{tabular}
  \caption[Comparision of balanced data]{Comparision of balanced data. The average precision, recall and F1-score for each balanced set. }
  \label{tab:balance}
\end{table}

Balancing the data turned out to make things worse.
Only when the unimportant part is four times larger than the important part the performance is the same as the unbalanced data.

Our last attempt to improve the performance was to a different feature selection.
A feature importance plot of one of the projects is shown in figure~\ref{fig:feature-importance}.
It shows very clear that the age is the dominant factor for the prediction model.
This is true for almost all our selected projects.
Based on this information we decided to see what happens if we disabled the age feature.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{../figs/mean-decrease-accuracy.pdf}
  \caption[Plot of feature importance]
   {Plot of feature importance. The age of the pull request is the dominant factor.}
  \label{fig:feature-importance}
\end{figure}

Again the feature selection didn't improve the performance of the Random Forest algorithm.
The results are shown in table~\ref{tab:feature-selection}.
Without the age feature the recall is less than with the feature.

\begin{table}
  \begin{tabular}{ l | c | c | c | c | c | c }
    Without & $\overline{p}$ & $\sigma(p)$ & $\overline{r}$ & $\sigma(r)$ & $\overline{F1}$ & $\sigma(F1)$ \\ \hline
    \hline
    Age & .6853 & .1079 & .4256 & .1647 & .5114 & .1463 \\
  \end{tabular}
  \caption[Feature selection]{Feature selection. The average precision, recall and F1-score for a model without some features. }
  \label{tab:feature-selection}
\end{table}

Despite the fact that it should be possible to further improve the prediction model and because this is only a preliminary study to prioritizing pull requests, we decided to go forward with the current model.

\subsection{Architecture}
\label{sec:architecture}

Our implementation of the service consists of several components.
Figure~\ref{fig:architecture} shows a global overview of the architecture.
The \prioritizer service uses two main data sources: GitHub and the \ghtorrent project.
The latter provides a message queue which can be used to subscribe to pull request events.
When such an event arrives the \emph{watcher} component of the \prioritizer is notified (1) and starts prioritizing the project (2).
When the \emph{analyzer} gets a prioritization request it fetches the list of PRs from GitHub (3).
At the same time it fetches the pull request contents to the local Git clone.
When the data is fetched the analyzer starts enriching the PR list with data from the local clone and \ghtorrent (4).
This step calculates and adds several of the features to the PRs.
The data is now ready to be processed by the \emph{predictor} (5) which gives a certain rank to the PRs.
After the ordered list is returned to the analyzer (6), the output is generated and available for the \emph{visualizer} (7).
Details about the different components are discussed in the following sections.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{../figs/architecture.pdf}
  \caption[Diagram of the architecture]
   {Diagram of the architecture. It shows the different data sources and components used by the \prioritizer service.}
  \label{fig:architecture}
\end{figure}

\begin{description}
\item[Watcher]
At the beginning of the chain we have the watcher, it is implemented in the Scala programming language.
The watcher continuously listens to pull request event from \ghtorrent via a RabbitMQ message queue.
Pull request events are triggered when a PR is assigned, unassigned, labeled, unlabeled, opened, closed, reopened, or synchronized.
The PR events of all public repositories on GitHub are received by the watcher.
Incoming events from repositories that are not currently tracked are discarded by the watcher.
If an event is received from a tracked repository the analyzer is invoked for that specific repository.

\item[Analyzer]
The component at the center of the service is the analyzer, it is also implemented in Scala.
When the analyzer kicks in, it receives a repository name from the watcher.
An up-to-date list of open PRs for that repository is fetched from GitHub through the GitHub \api.
In parallel the local git clone is updated with commits of the latest PRs.
At the time both processes are finished, the list with PRs is enriched with data from \ghtorrent as well as the local git clone.

The enrichment process adds e.g. information about the pull request author like the \emph{accept rate}, the \emph{contributor rate} and others described in section~\ref{sec:features}.
To retrieve information like the mergability and the \emph{pairwise conflicts} of the PRs the local git clone is used.
Because of the $O(n^2)$ nature of the pairwise conflict process, this is the most time consuming feature of the service.
After the enrichment process is done, the list is written to a repository specific cache.
This enables a faster enriching process in future runs, especially the pairwise conflict check benefits from the cache.

Some of the enrichment information (e.g. the mergability or number of comments) is also available directly via the GitHub \api.
However, this data is only present when requesting a single PR, whereas a request for a list of 100 PRs lacks this sort of data.
This means that if we want this data directly from GitHub we have to create one \api request per pull request.
When prioritizing PRs for a large amount of repositories it consumes also a large amount of \api requests and reaches GitHub's maximum of 5,000 requests/hour quickly.
So, our solution to this problem is to use \ghtorrent instead.

Before the final output is written to the disk, the predictor is invoked on the enriched PR list.
A list of ordered PRs is returned back from the predictor to the analyzer.
Details about this process are in the next paragraph.
The analyzer completes its process with writing the ordered list of PRs to the disk in JSON format, which will be picked up by the visualizer.

\item[Predictor]
The predictor assigns a rank to every pull request, it is partly written in Scala and partly in R.
When the predictor receiver a list of PRs it first checks if there is already a up-to-date prediction model.
If this is not the case, the first step is to train the prediction model on a set of PRs from the past.

The training process consists of two phases, first the historical data must be retrieved and secondly the actual model must be trained.
Information about closed pull request, their comments and their authors is harvested from \ghtorrent.
Once all data is fetched, the lifetime of each PR is divided into time windows to construct the training data as explained in section~\ref{sec:training}.
When the training set is complete it is passed to the R program which trains the prediction model.

The Random Forest machine learning algorithm, called in R, learns what features are important for dividing the PRs in two classifications (important vs. not important).
The resulting model is saved to the disk for later use when predicting the ranks.

Once we have a up-to-date prediction model, we can use it for ordering the PR list.
The list and the model are feeded to the prediction algorithm in R.
Using the prediction model each PR is given a probability that it belongs to the \emph{important} cluster.
A PR with a high probability means that it is more likely to receive an event or action (from the user) soon.
We interpret PRs with a high probability as PRs that need attention, therefore the ranked list of PRs is obtained by ordering the list on the importance probability from high to low.
The final list is then returned to the analyzer.

\item[Visualizer]
The visualizer is a user interface for the ordered list, it is a web application written in HTML and Javascript.
An example view of the interface can be seen in figure~\ref{fig:ui}.
To make the development faster and easier we used AngularJS, a Javascript framework.
The visualizer is a static website which accesses the JSON produced by the analyzer.
When a specific repository is requested by the user, the corresponsding JSON file is parsed and showed on the web page.

The list is automatically sorted on the rank outputted by the predictor.
However, the user is also able to sort or filter manually on different fields and features.
On GitHub has only a limited set of fields available to sort and filter on.
The visualizer interface provides more sort and filter options based on the features from section~\ref{sec:features} e.g. conflicts, author properties or size.
Sorting on something trivial as the size of a PR is not support on GitHub while many integrators use it as manual prioritization feature \cite{GZSD15}.

The interface tells users on which PR they have to focus first.
However, to actually take action the user still has to go to GitHub's interface.
To make it easier for the user each PR in the list has a link to the GitHub version.
Futhermore, if a user wants more information about a specific PR, he/she can click the PR to reveal more details about the features.
\end{description}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{../figs/interface.pdf}
  \caption[The user interface]
   {The user interface. It shows an ordered list of pull requests that need attention.}
  \label{fig:ui}
\end{figure}
